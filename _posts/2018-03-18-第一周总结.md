---
layout:     post
title:      "林方智第一周学习笔记"
subtitle:   "千里之行始于足下"
date:       2018-03-18 12:00:00
author:     "LamFZ"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - 生活
---

# 林方智第一周学习笔记
### 学习内容：
* 《机器学习实战》—周志华 第一、二章内容
* 高等数学 第一章内容

## 机器学习实战第一章 — 机器学习基础
概念：
>研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。简单来说，机器学习就是把无序的数据转换成有用的信息。

基本术语：
* 数据集
 * 样本
 * 属性
 * 属性值
 * 样本空间

* 学习任务
 * 分类
 * 回归

* 训练数据是否有标记消息
 * 监督学习
 * 无监督学习

NFL(No Free Lunch Theorem)定理:
当所有的问题出现的机会相同或所有问题同等重要时，无论学习算法是聪明还是笨拙，他们的**期望性能都是相同的**。
但是因为在日常生活中，每个样本的属性的分布是不均匀的。因此NFL定理的先要条件不满足，这个定理只是让我们认识到脱离实际问题空谈“什么学习算法更好”是毫无意义的。

#### K-临近算法
简单来说就是：近朱者赤近墨者黑。
* 优点
 * 精度高，对异常值不敏感，无数据输入假定
* 缺点
 * 计算复杂度高，空间复杂度高
* 适用范围：
 * 数值型和标称型
代码实现：
```python
def classify0(inX, dataSet, labels, k):
    #numpy函数shape[0]返回dataSet的行数
    dataSetSize = dataSet.shape[0]
    #在列向量方向上重复inX共1次(横向)，行向量方向上重复inX共dataSetSize次(纵向)
    diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet
    #二维特征相减后平方
    sqDiffMat = diffMat**2
    #sum()所有元素相加，sum(0)列相加，sum(1)行相加
    sqDistances = sqDiffMat.sum(axis=1)
    #开方，计算出距离
    distances = sqDistances**0.5
    #返回distances中元素从小到大排序后的索引值
    sortedDistIndices = distances.argsort()
    #定一个记录类别次数的字典
    classCount = {}
    for i in range(k):
        #取出前k个元素的类别
        voteIlabel = labels[sortedDistIndices[i]]
        #dict.get(key,default=None),字典的get()方法,返回指定键的值,如果值不在字典中返回默认值。
        #计算类别次数
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    #key=operator.itemgetter(1)根据字典的值进行排序
    #key=operator.itemgetter(0)根据字典的键进行排序
    #reverse降序排序字典
    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    #返回次数最多的类别,即所要分类的类别
    return sortedClassCount[0][0]
```
这个几乎是所有机器学习里面算法实现起来最简单，原理理解起来最容易的一个。

### 贝叶斯公式
>三人成虎与后验概率

查阅了多个资料以及多篇文章之后，对于贝叶斯公式有了一点点理解，但是还是不是很透彻。个人感觉贝叶斯公式就是因为观测的原因导致了概率的变化（听起来有点像薛定谔的猫）。
比如，一个色子，往天上一丢，掉在地上的一瞬间你偷瞄了一眼，哟！好像是六点！然后你决定猜六点，假设看走眼的概率是30％，问你猜中的概率是多少。
乍一看，猜对概率好像就是P（六点概率 没走眼概率）= 0.17X0.7 =0.119
其实非也，上面的那条公式是在事件A,B都为先验概率下才能这样算，但是在色子的点数已经确定的情况下，你偷瞄了一眼，这就是属于后验概率了。因此这种情况就要使用贝叶斯公式。
首先，一个色子掉在地上是六点的概率，如果不钻牛角尖就是P（6）=0.17，在没有观察的情况下，六点的概率就是六分之一了。但是，你偏偏看了一眼（后验概率），这个时候就要分成两种情况了，第一是六点你看对的情况P=0.17x0.7=0.119，一种猜错你看错的情况P=0.83x0.3=0.249 因此你猜对的概率是P=0.119/（0.119+0.249）= 0.323  所以因为你看多了一眼，你猜对的概率大大上升，上升的原因就是因为后验概率的存在。感觉后验概率在机器学习中有着举足轻重的关心，因为在训练以及验证的过程中都需要用到这个知识。

老师群中发的关于贝叶斯文献的后面部分内容，现在读起来还是有点迷糊。

参考文章：
[浅谈贝叶斯](http://www.xuyankun.cn/2017/05/13/bayes/)
[贝叶斯搜索集合](https://www.zhihu.com/search?type=content&q=%E8%B4%9D%E5%8F%B6%E6%96%AF)

# 总结
现在还是处于基础阶段，感觉先要把基础概念以及理论先理顺，并可以用自己方式表达出来。而且要克服对英语论文的陌生感以及恐惧感，查找国内外的相关基础文献。而且要分配好各项任务的时间，不要再最后时间完成任务。加紧对数学的学习，特别是概率论以及线性代数。因为稀疏表示以及压缩感知涉及了许多线性代数的知识。而现在主要的工作重心要放在考研上，毕竟现在里考研也没久了。千里之行始于足下，这就是本人第一周的总结。
